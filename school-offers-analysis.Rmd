---
title: "Analysis of Primary School Applications and Offers in Surrey and Hampshire Since 2014"
subtitle: "Schools Data Analysis"
author: "Clare Gibson"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    number_sections: true
    toc_float: true
    theme: cosmo
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE)
```

# Introduction
This document forms part of a project I am working on to analyse and present primary school admissions and performance data in Surrey and Hampshire in a meaningful way to parents looking to choose a school for their child. The full project, including all source data and scripts, can be found on [Github][1].

## Data Overview
In this analysis, I will explore data relating to primary school admissions in Surrey and Hampshire for academic years beginning September 2014. The data was taken from the [UK Government Explore Education Statistics service][4] and contains public sector information licensed under the [Open Government Licence v3.0][5]. Full details of the data sources used for this analysis can be found in the [README][2] file within the Github repo.

The CSV dataset is located [here][3].

The dataset includes the number of offers made to applicants for both primary and secondary school places to start in September 2021, methods of application and the proportion who received preferred offers of various type. A time series is included going back to 2014, when primary data was collected for the first time following the introduction of the first primary national offer day.

While the dataset does include data for both primary and secondary applications, and for every local authority in England, my analysis will focus on primary applications in Surrey and Hampshire.

Variable names and descriptions for this file are provided below:

|Variable name	|Variable description
|:---           |:---
|admission_numbers	|Number of admission places available
|applications_received	|Number of applications received
|first_preference_offers	|a. Number of first preference offers
|first_preference_offers_percent	|b. First preference rate
|NC_year_admission	|Year of admission
|no_of_preferences	|Number of preferences allowed
|no_offer	|m. Number made no offer
|no_offer_percent	|n. No offer rate
|non_preferred_offer	|k. Number of non preferred offers
|non_preferred_offer_percent	|l. Non preferred offer rate
|offers_to_nonapplicants	|Total number of offers to non applicants
|one_of_the_three_preference_offers	|g. Number of top three preference offers
|one_of_the_three_preference_offers_percent	|h. Top three preference rate
|online_applications	|Number of applications submitted online
|online_apps_percent	|Percentage of applications submitted online
|preferred_school_offer	|i. Number of preferred offers
|preferred_school_offer_percent	|j. Preferred offer rate
|school_phase	|School phase - collection phase
|schools_in_another_la_offer	|q. Number of offers in another LA
|schools_in_another_la_offer_percent	|r. Other LA offer rate
|schools_in_la_offer	|o. Number of offers in home LA
|schools_in_la_offer_percent	|p. Home LA offer rate
|second_preference_offers	|c. Number of second preference offers
|second_preference_offers_percent	|d. Second preference rate
|third_preference_offers	|e. Number of third preference offers
|third_preference_offers_percent	|f. Third preference rate

## R Setup
I will make use of several libraries during this analysis:
```{r libraries}
library(tidyverse)
library(janitor)
```

# Questions
I am primarily interested in answering the following questions with this data:

* What proportion of applicants typically receive their first preference place across England as a whole?
* What proportion of applicants typically receive any of their preferred places across England as a whole?
* How do Surrey and Hampshire compare against the national (English) average for number of first preference and any preference places offered?
* Has the number of applications in Surrey and Hampshire gone up or down since 2014?
* Has the number of available places kept pace with the number of applications in Surrey and Hampshire since 2014?
* Is there any relationship between the rate of online applications and the rate of first preference offers?

As I work through the data wrangling and exploration steps, I may uncover further questions that I wish to answer with this data.

# Data Wrangling
## Gather
First I need to read in the data from the [CSV file][3] so that I can explore, clean and analyse it further.
```{r read-offers}
r_offers_la <- read_csv("data-in/appsandoffers_2021.csv")
```

I use the prefix `r_` at the start of the variable name for the dataframe I just read in. This denotes the variable as being the 'raw' version of the data (i.e. exactly as it appeared at source). When I start to clean and wrangle this data, I will store the manipulated version as a new variable, without the `r_` prefix. In this way, I always have a copy of the raw data to refer back to if needed.

### Warnings
The output of the `read_csv()` code in the chunk above resulted in several warnings relating to parsing failures (the warnings are not displayed here but can be called using `problems(df)`. I will print the warnings into a friendly format to see what is going on. 

```{r read-problems, message=FALSE, warning=FALSE}
r_offers_la_problems <- problems(r_offers_la) %>% 
  group_by(col, expected, actual) %>% 
  summarise(nrows = n())
```

```{r kable-problems, echo=FALSE}
knitr::kable(r_offers_la_problems, caption="Table 2: Warnings displayed when reading in the applications and offers data")
```

Here I can see that there are 19 columns that the `read_csv()` function determined should be numeric based on analysis of the first 1,000 rows of data. However, in some rows it found a string `"c"` in these columns (as shown by the value in the `actual` column above).

From the material that accompanied the data in its [original source location][6] I know that some values were suppressed, and replaced with a string denoting the suppression:

>**Rounding**
>
>Primary and secondary suppression was applied in the 2014 dataset. No suppression has been applied from 2015 onwards
>
>**Conventions**
>
>The following convention is used throughout the underlying data.
>
>* ‘c’         Suppressed (2014 only) 
>* ‘:’         Not applicable.
>
>`r tufte::quote_footer('From [Explore Education Statistics][6] website')`

The suppressed values have been read into the R dataframe as null values (`NA`). As I work through the analysis I will need to decide if and how to impute these missing values.

## Assess
Now that I have read the data into R, I can spend some time assessing it to find out what the features look like and what might need to be cleaned.

I'll start by looking at a summary of the numerical columns of the dataset.

```{r offers-summary-num}
r_offers_la %>% 
  select(where(is.numeric)) %>% 
  summary()
```

From the summary I observe the following:

* The `timeperiod` column denotes an academic year, but is in the format '202021'. I would like to mutate this numeric column (a numeric column so that it displays only the start year of the academic year (i.e. '202021' becomes '2020'. I could also rename it to `year_of_entry` for clarity).
* The `old_la_code` column contains a categorical variable rather than a true numerical measure, and could benefit from being converted to a string.
* Several of the columns show a large difference between the mean and the median. Looking at the description of the dataset on the [source website][4], I see that it states: "Figures are provided at national, regional and local authority level." This means that there are some records that display an aggregation of other records within the dataset. To help my analysis, I will remove these aggregated records (I can easily recreate the aggregations from the remaining granular data if needed).
* As mentioned earlier, some of the columns have missing data and I will need to decide the best way to handle these missing values for my analyses.
* `online_apps_percent` has a large range (min: 0.06%, max: 100%). It might be interesting to see how this measure varies by local authority or over time.
* The columns containing percentages follow the format `97.6`, `88.3`, `3.2` etc. I would like to convert these to decimals (i.e. divide each value by 100) to make it easier to work with later.

I can also view a summary of the character columns.

```{r offers-summary-char}
r_offers_la %>% 
  select(where(is.character)) %>% 
  lapply(table)
```

From this I can make some further observations:

* Several columns appear to be redundant as they contain only a single value. `time_identifer`, `country_code`, `country_name` can probably all be removed since they contain no useful information.
* `NC_year_admission` and `no_of_preferences` both contain 'not applicable' values denoted by `:`. I believe these null values are tied to the records with the regional and national aggregated data and will therefore disappear when I remove those rows.
* `no_of_preferences` would be better converted to a numeric column once the 'not applicable' records have been removed.

## Clean
I'll start by fixing the issues I uncovered during my initial assessment. I will save my changes into a new dataframe `offers_la` so that I have a copy of both the raw data and my edited version.

```{r offers-convert-timeperiod-to-date}
# convert the timeperiod column to year_of_entry
offers_la <- r_offers_la %>% 
  mutate(time_period = as.character(time_period),
         time_period = str_sub(time_period, 1, 4),
         time_period = as.numeric(time_period)) %>% 
  rename(year_of_entry = time_period)

head(offers_la$year_of_entry)
```

```{r offers-convert-old-la-code-to-char}
# convert the old_la_code column to character
offers_la <- offers_la %>% 
  mutate(old_la_code = as.character(old_la_code))

tail(offers_la$old_la_code)
```

```{r offers-remove-agg-records}
# remove the aggregated data
offers_la <- offers_la %>% 
  filter(geographic_level == "Local authority")
```

```{r offers-convert-percentages}
# convert percentages to decimal
offers_la <- offers_la %>% 
  mutate(across(ends_with("_percent"), ~ .x / 100))

offers_la %>% 
  select(ends_with("_percent")) %>% 
  head()
```

```{r offers-remove-redundant-cols}
# remove redundant columns
offers_la <- offers_la %>% 
  select(!c(time_identifier, country_code, country_name))
```

```{r offers-convert-no-pref-to-char}
offers_la <- offers_la %>% 
  mutate(no_of_preferences = as.numeric(no_of_preferences))
```

### Handling Missing Values
I have several columns that contain missing values, and I need to decide if and how to impute these values. I'll start by inspecting the clean dataframe to find out where the missing values are located.

```{r offers-location-of-missing-values}
knitr::kable(colSums(is.na(offers_la)),
             col.names = c("count_na"))
```

# Export Clean Data
I will save my clean data to a new CSV file that I can use for visualization in Tableau.

```{r offers-export-csv}
write_csv(offers_la, file="data-out/offers-la.csv", na="")
```


[1]: <https://github.com/clarelgibson/schools> "Github repo"
[2]: <https://github.com/clarelgibson/schools#readme> "README"
[3]: <https://github.com/clarelgibson/schools/blob/94a59bbf3c5a361a437e50ff88b34fc7d4dace09/data-in/appsandoffers_2021.csv> "Admissions data"
[4]: <https://explore-education-statistics.service.gov.uk/find-statistics/secondary-and-primary-school-applications-and-offers#dataDownloads-1> "Applications and Offers"
[5]: <https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/> "OGL v3.0"
[6]: <https://explore-education-statistics.service.gov.uk/find-statistics/secondary-and-primary-school-applications-and-offers/meta-guidance> "Applications and Offers Meta Guidance"